{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b48d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Model evaluation and tuning\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                           precision_recall_curve, roc_curve, auc, accuracy_score,\n",
    "                           precision_score, recall_score, f1_score, average_precision_score)\n",
    "\n",
    "# Additional utilities\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f9a2cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ CUSTOMER CHURN ANALYSIS - PART 4\n",
      "================================================================================\n",
      "üéØ Machine Learning Modeling & Evaluation\n",
      "================================================================================\n",
      "üìÖ Modeling Date: 2025-09-16 21:51\n",
      "üéì Learning Focus: Build, tune, and evaluate ML models\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Set up plotting configuration\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../images', exist_ok=True)\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "print(\"ü§ñ CUSTOMER CHURN ANALYSIS - PART 4\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ Machine Learning Modeling & Evaluation\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìÖ Modeling Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(\"üéì Learning Focus: Build, tune, and evaluate ML models\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca594c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ STEP 1: LOADING PROCESSED DATA\n",
      "----------------------------------------\n",
      "‚úÖ Datasets loaded successfully!\n",
      "   ‚Ä¢ Training set: (5634, 39)\n",
      "   ‚Ä¢ Validation set: (1409, 39)\n",
      "   ‚Ä¢ Features: 39\n",
      "   ‚Ä¢ Training churn rate: 26.5%\n",
      "   ‚Ä¢ Validation churn rate: 26.5%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìÇ STEP 1: LOADING PROCESSED DATA\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load the processed datasets\n",
    "try:\n",
    "    X_train = pd.read_csv('../processed_data/X_train.csv')\n",
    "    X_val = pd.read_csv('../processed_data/X_val.csv')\n",
    "    y_train = pd.read_csv('../processed_data/y_train.csv').squeeze()\n",
    "    y_val = pd.read_csv('../processed_data/y_val.csv').squeeze()\n",
    "    \n",
    "    # Load feature names\n",
    "    with open('../processed_data/feature_names.txt', 'r') as f:\n",
    "        feature_names = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    print(\"‚úÖ Datasets loaded successfully!\")\n",
    "    print(f\"   ‚Ä¢ Training set: {X_train.shape}\")\n",
    "    print(f\"   ‚Ä¢ Validation set: {X_val.shape}\")\n",
    "    print(f\"   ‚Ä¢ Features: {len(feature_names)}\")\n",
    "    print(f\"   ‚Ä¢ Training churn rate: {y_train.mean():.1%}\")\n",
    "    print(f\"   ‚Ä¢ Validation churn rate: {y_val.mean():.1%}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\"Please ensure you've run 03_preprocessing_feature_engineering.ipynb first\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cbb1ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ STEP 2: MODEL SETUP & BASELINE\n",
      "-----------------------------------\n",
      "ü§ñ Models to evaluate: 6\n",
      "   1. Logistic Regression\n",
      "   2. Random Forest\n",
      "   3. Gradient Boosting\n",
      "   4. SVM\n",
      "   5. Neural Network\n",
      "   6. Naive Bayes\n",
      "\n",
      "üìä Baseline Performance:\n",
      "   ‚Ä¢ Majority class accuracy: 73.5%\n",
      "   ‚Ä¢ Random guess accuracy: ~50%\n",
      "   ‚Ä¢ Target: Beat 73.5% significantly\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüéØ STEP 2: MODEL SETUP & BASELINE\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Neural Network': MLPClassifier(random_state=42, max_iter=1000),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "print(f\"ü§ñ Models to evaluate: {len(models)}\")\n",
    "for i, model_name in enumerate(models.keys(), 1):\n",
    "    print(f\"   {i}. {model_name}\")\n",
    "\n",
    "# Create baseline prediction (majority class)\n",
    "baseline_accuracy = max(y_train.mean(), 1 - y_train.mean())\n",
    "print(f\"\\nüìä Baseline Performance:\")\n",
    "print(f\"   ‚Ä¢ Majority class accuracy: {baseline_accuracy:.1%}\")\n",
    "print(f\"   ‚Ä¢ Random guess accuracy: ~50%\")\n",
    "print(f\"   ‚Ä¢ Target: Beat {baseline_accuracy:.1%} significantly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6fae4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab6cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüèÉ STEP 3: INITIAL MODEL TRAINING\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"üîÑ Training models...\")\n",
    "for model_name, model in models.items():\n",
    "    print(f\"   Training {model_name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[model_name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'PR-AUC': pr_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Initial training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395478a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä STEP 4: MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.drop(['y_pred', 'y_pred_proba'], axis=1)\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"üèÜ Model Performance Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Find best model for each metric\n",
    "best_models = {}\n",
    "for metric in results_df.columns:\n",
    "    best_model = results_df[metric].idxmax()\n",
    "    best_score = results_df[metric].max()\n",
    "    best_models[metric] = {'model': best_model, 'score': best_score}\n",
    "    \n",
    "print(f\"\\nü•á Best Models by Metric:\")\n",
    "print(\"-\" * 25)\n",
    "for metric, info in best_models.items():\n",
    "    print(f\"   ‚Ä¢ {metric:<12}: {info['model']} ({info['score']:.3f})\")\n",
    "\n",
    "# Overall best model (balanced performance)\n",
    "results_df['Overall_Score'] = (results_df['F1-Score'] + results_df['ROC-AUC'] + results_df['PR-AUC']) / 3\n",
    "best_overall = results_df['Overall_Score'].idxmax()\n",
    "print(f\"\\nüèÜ Best Overall Model: {best_overall} (Score: {results_df.loc[best_overall, 'Overall_Score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea9fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìà STEP 5: MODEL COMPARISON VISUALIZATIONS\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# 1. Performance metrics comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'PR-AUC']\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum', 'orange']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    row, col = divmod(i, 3)\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    metric_scores = results_df[metric].sort_values(ascending=False)\n",
    "    bars = ax.bar(range(len(metric_scores)), metric_scores.values, color=colors[i], alpha=0.8)\n",
    "    \n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xticks(range(len(metric_scores)))\n",
    "    ax.set_xticklabels(metric_scores.index, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for j, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. ROC Curves comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "for model_name in models.keys():\n",
    "    y_pred_proba = results[model_name]['y_pred_proba']\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "    auc_score = roc_auc_score(y_val, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC: {auc_score:.3f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison', fontsize=16, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../images/roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. Precision-Recall Curves comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "for model_name in models.keys():\n",
    "    y_pred_proba = results[model_name]['y_pred_proba']\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_val, y_pred_proba)\n",
    "    plt.plot(recall, precision, label=f'{model_name} (PR-AUC: {pr_auc:.3f})', linewidth=2)\n",
    "\n",
    "plt.axhline(y=y_val.mean(), color='k', linestyle='--', alpha=0.5, label=f'Baseline ({y_val.mean():.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves Comparison', fontsize=16, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../images/precision_recall_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3bf329",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüîß STEP 6: HYPERPARAMETER TUNING - {best_overall}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Define parameter grids for top models\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning for the best model\n",
    "if best_overall in param_grids:\n",
    "    print(f\"üîç Tuning {best_overall}...\")\n",
    "    \n",
    "    # Set up cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        models[best_overall], \n",
    "        param_grids[best_overall],\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"   Running grid search...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best model\n",
    "    best_tuned_model = grid_search.best_estimator_\n",
    "    print(f\"‚úÖ Best parameters found:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"   ‚Ä¢ {param}: {value}\")\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    y_pred_tuned = best_tuned_model.predict(X_val)\n",
    "    y_pred_proba_tuned = best_tuned_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    tuned_f1 = f1_score(y_val, y_pred_tuned)\n",
    "    tuned_roc_auc = roc_auc_score(y_val, y_pred_proba_tuned)\n",
    "    \n",
    "    print(f\"\\nüìà Performance Improvement:\")\n",
    "    print(f\"   ‚Ä¢ Original F1-Score: {results[best_overall]['F1-Score']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Tuned F1-Score: {tuned_f1:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Improvement: {tuned_f1 - results[best_overall]['F1-Score']:.3f}\")\n",
    "    \n",
    "    # Update best model\n",
    "    final_model = best_tuned_model\n",
    "    final_predictions = y_pred_tuned\n",
    "    final_probabilities = y_pred_proba_tuned\n",
    "    \n",
    "else:\n",
    "    print(f\"Using original {best_overall} model\")\n",
    "    final_model = trained_models[best_overall]\n",
    "    final_predictions = results[best_overall]['y_pred']\n",
    "    final_probabilities = results[best_overall]['y_pred_proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüîç STEP 7: DETAILED ANALYSIS - {best_overall}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val, final_predictions)\n",
    "print(\"üìä Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(y_val, final_predictions, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Churn', 'Churn'], \n",
    "            yticklabels=['No Churn', 'Churn'])\n",
    "plt.title(f'Confusion Matrix - {best_overall}', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.savefig('../images/confusion_matrix_best_model.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f426ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ STEP 8: FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "# Get feature importance based on model type\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    # Tree-based models\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': final_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"üèÜ Top 15 Most Important Features:\")\n",
    "    print(\"-\" * 35)\n",
    "    for i, (_, row) in enumerate(feature_importance.head(15).iterrows(), 1):\n",
    "        print(f\"   {i:2d}. {row['Feature']:<25}: {row['Importance']:.4f}\")\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'], color='skyblue', alpha=0.8)\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 15 Feature Importances - {best_overall}', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../images/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "elif hasattr(final_model, 'coef_'):\n",
    "    # Linear models\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': final_model.coef_[0],\n",
    "        'Abs_Coefficient': np.abs(final_model.coef_[0])\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"üèÜ Top 15 Most Important Features (by |coefficient|):\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (_, row) in enumerate(feature_importance.head(15).iterrows(), 1):\n",
    "        direction = \"‚Üë\" if row['Coefficient'] > 0 else \"‚Üì\"\n",
    "        print(f\"   {i:2d}. {row['Feature']:<25}: {row['Coefficient']:>7.4f} {direction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb36895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüí∞ STEP 9: BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"-\" * 38)\n",
    "\n",
    "# Define business costs (example values - adjust based on real business)\n",
    "cost_acquire_customer = 100  # Cost to acquire a new customer\n",
    "value_retained_customer = 500  # Value of retaining a customer for a year\n",
    "cost_retention_campaign = 50   # Cost of retention campaign per customer\n",
    "\n",
    "# Calculate business metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Business scenarios\n",
    "total_customers = len(y_val)\n",
    "actual_churners = (y_val == 1).sum()\n",
    "predicted_churners = (final_predictions == 1).sum()\n",
    "\n",
    "print(f\"üìä Business Scenario Analysis:\")\n",
    "print(f\"   ‚Ä¢ Total customers in validation: {total_customers:,}\")\n",
    "print(f\"   ‚Ä¢ Actual churners: {actual_churners:,} ({actual_churners/total_customers:.1%})\")\n",
    "print(f\"   ‚Ä¢ Predicted churners: {predicted_churners:,} ({predicted_churners/total_customers:.1%})\")\n",
    "\n",
    "# Cost-Benefit Analysis\n",
    "print(f\"\\nüí° Model Business Impact:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Without model (do nothing)\n",
    "cost_without_model = actual_churners * cost_acquire_customer\n",
    "print(f\"   ‚Ä¢ Cost without model (lost customers): ${cost_without_model:,}\")\n",
    "\n",
    "# With model\n",
    "# True Positives: Correctly identified churners we can save\n",
    "revenue_saved = tp * (value_retained_customer - cost_retention_campaign)\n",
    "# False Positives: Unnecessary retention campaigns  \n",
    "wasted_campaigns = fp * cost_retention_campaign\n",
    "# False Negatives: Churners we missed\n",
    "missed_opportunities = fn * cost_acquire_customer\n",
    "\n",
    "net_benefit = revenue_saved - wasted_campaigns - missed_opportunities\n",
    "total_cost_with_model = wasted_campaigns + missed_opportunities\n",
    "\n",
    "print(f\"   ‚Ä¢ Revenue from saved customers (TP): ${revenue_saved:,}\")\n",
    "print(f\"   ‚Ä¢ Cost of unnecessary campaigns (FP): ${wasted_campaigns:,}\")\n",
    "print(f\"   ‚Ä¢ Cost of missed churners (FN): ${missed_opportunities:,}\")\n",
    "print(f\"   ‚Ä¢ Total cost with model: ${total_cost_with_model:,}\")\n",
    "print(f\"   ‚Ä¢ Net benefit: ${net_benefit:,}\")\n",
    "print(f\"   ‚Ä¢ ROI: {(net_benefit / max(total_cost_with_model, 1)) * 100:.1f}%\")\n",
    "\n",
    "savings = cost_without_model - total_cost_with_model\n",
    "print(f\"\\nüéØ Bottom Line: ${savings:,} saved vs doing nothing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48588790",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ STEP 10: MODEL DEPLOYMENT PREPARATION\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Save the best model\n",
    "model_filename = f'../models/best_churn_model_{best_overall.lower().replace(\" \", \"_\")}.pkl'\n",
    "joblib.dump(final_model, model_filename)\n",
    "print(f\"‚úÖ Model saved: {model_filename}\")\n",
    "\n",
    "# Save feature names and preprocessing info\n",
    "deployment_info = {\n",
    "    'model_name': best_overall,\n",
    "    'features': feature_names,\n",
    "    'feature_count': len(feature_names),\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'performance_metrics': {\n",
    "        'accuracy': accuracy_score(y_val, final_predictions),\n",
    "        'precision': precision_score(y_val, final_predictions),\n",
    "        'recall': recall_score(y_val, final_predictions),\n",
    "        'f1_score': f1_score(y_val, final_predictions),\n",
    "        'roc_auc': roc_auc_score(y_val, final_probabilities)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save deployment info\n",
    "import json\n",
    "with open('../models/model_deployment_info.json', 'w') as f:\n",
    "    json.dump(deployment_info, f, indent=4, default=str)\n",
    "\n",
    "print(\"‚úÖ Deployment info saved: model_deployment_info.json\")\n",
    "\n",
    "# Create prediction function template\n",
    "prediction_template = f'''\n",
    "def predict_churn(customer_data):\n",
    "    \"\"\"\n",
    "    Predict customer churn probability\n",
    "    \n",
    "    Parameters:\n",
    "    customer_data: dict or DataFrame with features: {feature_names[:5]}... (total: {len(feature_names)})\n",
    "    \n",
    "    Returns:\n",
    "    probability: float (0-1) - probability of churn\n",
    "    prediction: int (0/1) - binary prediction\n",
    "    \"\"\"\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    \n",
    "    model = joblib.load('best_churn_model_{best_overall.lower().replace(\" \", \"_\")}.pkl')\n",
    "    \n",
    "    if isinstance(customer_data, dict):\n",
    "        customer_data = pd.DataFrame([customer_data])\n",
    "    \n",
    "    probability = model.predict_proba(customer_data)[0][1]\n",
    "    prediction = model.predict(customer_data)[0]\n",
    "    \n",
    "    return {{\n",
    "        'churn_probability': probability,\n",
    "        'will_churn': bool(prediction),\n",
    "        'risk_level': 'High' if probability > 0.7 else 'Medium' if probability > 0.3 else 'Low'\n",
    "    }}\n",
    "'''\n",
    "\n",
    "with open('../models/prediction_template.py', 'w') as f:\n",
    "    f.write(prediction_template)\n",
    "\n",
    "print(\"‚úÖ Prediction template saved: prediction_template.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìã FINAL SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "print(f\"üèÜ BEST MODEL: {best_overall}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Performance Metrics:\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {accuracy_score(y_val, final_predictions):.1%}\")\n",
    "print(f\"   ‚Ä¢ Precision: {precision_score(y_val, final_predictions):.1%}\")\n",
    "print(f\"   ‚Ä¢ Recall: {recall_score(y_val, final_predictions):.1%}\")\n",
    "print(f\"   ‚Ä¢ F1-Score: {f1_score(y_val, final_predictions):.3f}\")\n",
    "print(f\"   ‚Ä¢ ROC-AUC: {roc_auc_score(y_val, final_probabilities):.3f}\")\n",
    "\n",
    "print(f\"\\nüí∞ Business Impact:\")\n",
    "print(f\"   ‚Ä¢ Estimated savings: ${savings:,}\")\n",
    "print(f\"   ‚Ä¢ ROI: {(net_benefit / max(total_cost_with_model, 1)) * 100:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Customers saved: {tp} out of {actual_churners}\")\n",
    "\n",
    "print(f\"\\nüéØ Key Insights:\")\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    top_3_features = feature_importance.head(3)['Feature'].tolist()\n",
    "    print(f\"   ‚Ä¢ Top churn predictors: {', '.join(top_3_features)}\")\n",
    "print(f\"   ‚Ä¢ Model can identify {recall_score(y_val, final_predictions):.1%} of churners\")\n",
    "print(f\"   ‚Ä¢ {precision_score(y_val, final_predictions):.1%} of predicted churners actually churn\")\n",
    "\n",
    "print(f\"\\nüìã Business Recommendations:\")\n",
    "print(\"   1. üéØ Target high-risk customers (probability > 0.7) with retention campaigns\")\n",
    "print(\"   2. üìû Focus on contract renewals for month-to-month customers\")\n",
    "print(\"   3. üí≥ Encourage automatic payment methods\")\n",
    "print(\"   4. üìä Monitor model performance monthly and retrain quarterly\")\n",
    "print(\"   5. üîÑ A/B test retention strategies on predicted churners\")\n",
    "\n",
    "print(f\"\\nüìÅ Deliverables Created:\")\n",
    "print(\"   ‚Ä¢ Trained and validated ML model\")\n",
    "print(\"   ‚Ä¢ Feature importance analysis\")\n",
    "print(\"   ‚Ä¢ Business impact assessment\")\n",
    "print(\"   ‚Ä¢ Model deployment package\")\n",
    "print(\"   ‚Ä¢ Comprehensive evaluation reports\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
